{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "44859ca3-13a5-44de-a2e8-359ffc3e1d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.nnx as nnx\n",
    "import einops\n",
    "\n",
    "\"\"\"\n",
    "Dimension key:\n",
    "B: batch size\n",
    "L: sequence length\n",
    "D: embedding dimension\n",
    "H: number of attention heads\n",
    "K: size of each attention head (D // H)\n",
    "\"\"\"\n",
    "\n",
    "class SelfAttention(nnx.Module):\n",
    "    def __init__(self, n_heads: int, d_embed: int, rngs: nnx.Rngs, in_proj_bias: bool = True, out_proj_bias: bool = True):\n",
    "        self.n_heads = n_heads\n",
    "        self.d_embed = d_embed\n",
    "        self.d_head = d_embed // n_heads\n",
    "        self.in_proj = nnx.Linear(in_features=d_embed, out_features=3*d_embed, use_bias=in_proj_bias, rngs=rngs)\n",
    "        self.out_proj = nnx.Linear(in_features=d_embed, out_features=d_embed, use_bias=out_proj_bias, rngs=rngs)\n",
    "\n",
    "    def __call__(self, x_BLD,  causal_mask=False):\n",
    "        B, L, D = jnp.shape(x_BLD)\n",
    "        H = self.n_heads\n",
    "        K = self.d_head # K = D // H\n",
    "\n",
    "        qkv_BL3D = self.in_proj(x_BLD) # (B, L, 3D)\n",
    "        q_BLD, k_BLD, v_BLD = jnp.split(qkv_BL3D, 3, axis=-1)\n",
    "\n",
    "        q_BHLK = einops.rearrange(q_BLD, 'B L (H K) -> B H L K', H=H)\n",
    "        k_BHLK = einops.rearrange(k_BLD, 'B L (H K) -> B H L K', H=H)\n",
    "        v_BHLK = einops.rearrange(v_BLD, 'B L (H K) -> B H L K', H=H)\n",
    "\n",
    "        attn_logits_BHLL = jnp.einsum('BHLK,BHMK->BHLM', q_BHLK, k_BHLK) / jnp.sqrt(K)\n",
    "\n",
    "        if causal_mask:\n",
    "            mask_LL = jnp.triu(jnp.ones((L, L)), k=1).astype(bool)\n",
    "            attn_logits_BHLL = jnp.where(mask_LL, -jnp.inf, attn_logits_BHLL)\n",
    "\n",
    "        attn_weights_BHLL = nnx.softmax(attn_logits_BHLL, axis=-1)\n",
    "\n",
    "        out_BHLK = jnp.einsum('BHLM,BHMK->BHLK', attn_logits_BHLL, v_BHLK)\n",
    "\n",
    "        out_BLD = einops.rearrange(out_BHLK, 'B H L K -> B L (H K)')\n",
    "\n",
    "        out_BLD = self.out_proj(out_BLD)\n",
    "\n",
    "        return out_BLD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000b5b67-ffdc-458f-8155-8478b0119e8a",
   "metadata": {},
   "source": [
    "# Test attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "dab216c4-bad5-41e2-a5d5-eb71de104714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfAttention shape test passed!\n"
     ]
    }
   ],
   "source": [
    "def test_self_attention_shape():\n",
    "    B, L, D, H = 8, 10, 64, 8\n",
    "    rngs = nnx.Rngs({'params': jax.random.PRNGKey(0)})\n",
    "\n",
    "    attention_layer = SelfAttention(n_heads=H, d_embed=D, rngs=rngs)\n",
    "    x_BLD = jax.random.normal(jax.random.PRNGKey(1), (B, L, D))\n",
    "    output = attention_layer(x_BLD, causal_mask=True)\n",
    "    assert output.shape == (B, L, D), f\"Expected shape {(B, L, D)}, but got {output.shape}\"\n",
    "    print(\"SelfAttention shape test passed!\")\n",
    "\n",
    "# Run the shape test\n",
    "test_self_attention_shape()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "45063b19-3909-4411-958c-aa203b7b9ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flax.nnx as nnx\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import einops\n",
    "\n",
    "\"\"\"\n",
    "Dimension key:\n",
    "(missing!)\n",
    "\"\"\"\n",
    "\n",
    "class ResidualBlock:\n",
    "    def __init__(self, in_features: int, out_features: int, rngs: nnx.Rngs):\n",
    "        self.gn1 = nnx.GroupNorm(num_features=in_features, num_groups=32, rngs=rngs)\n",
    "        self.conv1 = nnx.Conv(in_features=in_features, out_features=out_features, kernel_size=(3,3), padding=1, rngs=rngs)\n",
    "\n",
    "        self.gn2 = nnx.GroupNorm(num_features=out_features, num_groups=32, rngs=rngs)\n",
    "        self.conv2 = nnx.Conv(in_features=out_features, out_features=out_features, kernel_size=(3,3), padding=1, rngs=rngs)\n",
    "\n",
    "        if in_features != out_features:\n",
    "            self.shortcut = nnx.Conv(in_features=in_features, out_features=out_features, kernel_size=(1,1), padding=0, rngs=rngs)\n",
    "        else:\n",
    "            self.shortcut = lambda x: x\n",
    "\n",
    "    def __call__(self, x):\n",
    "        residual = x\n",
    "\n",
    "        x = self.gn1(x)\n",
    "        x = nnx.silu(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.gn2(x)\n",
    "        x = nnx.silu(x)\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        return x + self.shortcut(residual)\n",
    "\n",
    "class AttentionBlock:\n",
    "    def __init__(self, num_features: int, rngs: nnx.Rngs):\n",
    "        self.gn = nnx.GroupNorm(num_features=num_features, num_groups=32, rngs=rngs)\n",
    "        self.attention = SelfAttention(n_heads=1, d_embed=num_features, rngs=rngs)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # recall that JAX expects (B, H, W, C)\n",
    "        residual = x\n",
    "\n",
    "        x = self.gn(x)\n",
    "\n",
    "        B, H, W, C = jnp.shape(x)\n",
    "\n",
    "        x = einops.rearrange(x, 'B H W C -> B (H W) C')\n",
    "        x = self.attention(x)\n",
    "        x = einops.rearrange(x, 'B (H W) C -> B H W C', H=H, W=W)\n",
    "        x += residual\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e082b0-0ab4-4054-ab60-504c2dc469c2",
   "metadata": {},
   "source": [
    "# Test residual block and attention block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5bc82637-ec8d-4e50-bf2a-34e87d19aee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResidualBlock shape test passed!\n"
     ]
    }
   ],
   "source": [
    "def test_residual_block_shape():\n",
    "    B, H, W, in_features, out_features = 8, 32, 32, 64, 128  # Example: batch size 8, height and width 32, input channels 64, output channels 128\n",
    "    rngs = nnx.Rngs({'params': jax.random.PRNGKey(0)})\n",
    "    \n",
    "    # Instantiate the ResidualBlock\n",
    "    residual_block = ResidualBlock(in_features=in_features, out_features=out_features, rngs=rngs)\n",
    "\n",
    "    # Random input tensor with shape (B, H, W, in_features)\n",
    "    x = jax.random.normal(jax.random.PRNGKey(1), (B, H, W, in_features))\n",
    "\n",
    "    # Forward pass through the ResidualBlock\n",
    "    output = residual_block(x)\n",
    "\n",
    "    # Check the shape of the output\n",
    "    assert output.shape == (B, H, W, out_features), f\"Expected shape {(B, H, W, out_features)}, but got {output.shape}\"\n",
    "\n",
    "    print(\"ResidualBlock shape test passed!\")\n",
    "\n",
    "# Run the shape test\n",
    "test_residual_block_shape()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4d7406ab-24bb-46fe-abf9-dfecff8cd27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AttentionBlock shape test passed!\n"
     ]
    }
   ],
   "source": [
    "def test_attention_block_shape():\n",
    "    B, H, W, C = 8, 32, 32, 64 \n",
    "    rngs = nnx.Rngs({'params': jax.random.PRNGKey(0)})\n",
    "    attention_block = AttentionBlock(num_features=C, rngs=rngs)\n",
    "    x = jax.random.normal(jax.random.PRNGKey(1), (B, H, W, C))\n",
    "    output = attention_block(x)\n",
    "    assert output.shape == (B, H, W, C), f\"Expected shape {(B, H, W, C)}, but got {output.shape}\"\n",
    "\n",
    "    print(\"AttentionBlock shape test passed!\")\n",
    "\n",
    "# Run the shape test\n",
    "test_attention_block_shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6a5693-12d9-419b-986c-1de222ec43a4",
   "metadata": {},
   "source": [
    "# Test encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "cbcb6b07-f02c-4467-b861-2c4a197058f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nnx.Module):\n",
    "    def __init__(self, rngs: nnx.Rngs):\n",
    "        # 3 -> 128 -> (6 x 128)\n",
    "        self.conv1 = nnx.Conv(in_features=3, out_features=128, kernel_size=(3, 3), padding=1, rngs=rngs)\n",
    "\n",
    "        self.res1 = ResidualBlock(in_features=128, out_features=128, rngs=rngs)\n",
    "        self.res2 = ResidualBlock(in_features=128, out_features=128, rngs=rngs)\n",
    "\n",
    "        self.conv2 = nnx.Conv(in_features=128, out_features=128, kernel_size=(3, 3), strides=(2, 2), padding=0, rngs=rngs)\n",
    "\n",
    "        # 128 -> 256 -> (4 x 256)\n",
    "        self.res3 = ResidualBlock(in_features=128, out_features=256, rngs=rngs)\n",
    "        self.res4 = ResidualBlock(in_features=256, out_features=256, rngs=rngs)\n",
    "\n",
    "        self.conv3 = nnx.Conv(in_features=256, out_features=256, kernel_size=(3, 3), strides=(2, 2), padding=0, rngs=rngs)\n",
    "\n",
    "        # 256 -> 512 -> (10 x 512)\n",
    "        self.res5 = ResidualBlock(in_features=256, out_features=512, rngs=rngs)\n",
    "        self.res6 = ResidualBlock(in_features=512, out_features=512, rngs=rngs)\n",
    "\n",
    "        self.conv4 = nnx.Conv(in_features=512, out_features=512, kernel_size=(3, 3), strides=(2, 2), padding=0, rngs=rngs)\n",
    "\n",
    "        self.res7 = ResidualBlock(in_features=512, out_features=512, rngs=rngs)\n",
    "        self.res8 = ResidualBlock(in_features=512, out_features=512, rngs=rngs)\n",
    "        self.res9 = ResidualBlock(in_features=512, out_features=512, rngs=rngs)\n",
    "\n",
    "        self.attention = AttentionBlock(num_features=512, rngs=rngs)\n",
    "\n",
    "        # 512 -> 512\n",
    "        self.res10 = ResidualBlock(in_features=512, out_features=512, rngs=rngs)\n",
    "\n",
    "        self.groupnorm = nnx.GroupNorm(num_features=512, num_groups=32, rngs=rngs)\n",
    "\n",
    "        # 512 -> 8 -> (2 x 8)\n",
    "        self.conv5 = nnx.Conv(in_features=512, out_features=8, kernel_size=(3, 3), padding=1, rngs=rngs)\n",
    "        self.conv6 = nnx.Conv(in_features=8, out_features=8, kernel_size=(1, 1), padding=0, rngs=rngs)\n",
    "\n",
    "    def __call__(self, x, noise):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        x: (B, H, W, C=3)\n",
    "        noise: (B, H/8, W/8, C=4)\n",
    "\n",
    "        Returns:\n",
    "        latent (z): (B, H/8, W/8, 4)\n",
    "        \"\"\"\n",
    "\n",
    "        # Q: is there a better way to do this?\n",
    "        for module in [self.conv1, self.res1, self.res2, self.conv2, self.res3, self.res4, self.conv3,\n",
    "                       self.res5, self.res6, self.conv4, self.res7, self.res8, self.res9, self.attention,\n",
    "                       self.res10, self.groupnorm]:\n",
    "\n",
    "            if isinstance(module, nnx.Conv) and module.strides == (2, 2):\n",
    "                x = pad_asymmetric(x,  padding=(1, 1))\n",
    "            x = module(x)\n",
    "\n",
    "        x = nnx.silu(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.conv6(x)\n",
    "\n",
    "        if x.shape[-1] % 2 != 0:\n",
    "            raise ValueError(\"Final output channels must be divisible by 2 to split mean and log_var\")\n",
    "\n",
    "        mean, log_var = jnp.split(x, 2, axis=-1)\n",
    "        log_var = jnp.clip(log_var, -30, 20)\n",
    "        variance = jnp.exp(log_var)\n",
    "        stdev = jnp.sqrt(variance)\n",
    "\n",
    "        x = mean + stdev * noise\n",
    "        x *= 0.18215\n",
    "\n",
    "        return x\n",
    "\n",
    "class Decoder(nnx.Module):\n",
    "    def __init__(self, rngs: nnx.Rngs):\n",
    "        # Initial Convolutions and Blocks\n",
    "        self.conv1 = nnx.Conv(in_features=4, out_features=4, kernel_size=(1, 1), padding=0, rngs=rngs)\n",
    "        self.conv2 = nnx.Conv(in_features=4, out_features=512, kernel_size=(3, 3), padding=1, rngs=rngs)\n",
    "        self.res1 = ResidualBlock(in_features=512, out_features=512, rngs=rngs)\n",
    "        self.attention = AttentionBlock(num_features=512, rngs=rngs)\n",
    "        self.res2 = ResidualBlock(in_features=512, out_features=512, rngs=rngs)\n",
    "        self.res3 = ResidualBlock(in_features=512, out_features=512, rngs=rngs)\n",
    "        self.res4 = ResidualBlock(in_features=512, out_features=512, rngs=rngs)\n",
    "        self.res5 = ResidualBlock(in_features=512, out_features=512, rngs=rngs)\n",
    "\n",
    "        # Upsampling Blocks\n",
    "        self.conv3 = nnx.Conv(in_features=512, out_features=512, kernel_size=(3, 3), padding=1, rngs=rngs)\n",
    "        self.res6 = ResidualBlock(in_features=512, out_features=512, rngs=rngs)\n",
    "        self.res7 = ResidualBlock(in_features=512, out_features=512, rngs=rngs)\n",
    "        self.res8 = ResidualBlock(in_features=512, out_features=512, rngs=rngs)\n",
    "\n",
    "        # Upsampling Blocks\n",
    "        self.conv4 = nnx.Conv(in_features=512, out_features=256, kernel_size=(3, 3), padding=1, rngs=rngs)\n",
    "        self.res9 = ResidualBlock(in_features=256, out_features=256, rngs=rngs)\n",
    "        self.res10 = ResidualBlock(in_features=256, out_features=256, rngs=rngs)\n",
    "        self.res11 = ResidualBlock(in_features=256, out_features=256, rngs=rngs)\n",
    "\n",
    "        # Upsampling Blocks\n",
    "        self.conv5 = nnx.Conv(in_features=256, out_features=128, kernel_size=(3, 3), padding=1, rngs=rngs)\n",
    "        self.res12 = ResidualBlock(in_features=128, out_features=128, rngs=rngs)\n",
    "        self.res13 = ResidualBlock(in_features=128, out_features=128, rngs=rngs)\n",
    "        self.res14 = ResidualBlock(in_features=128, out_features=128, rngs=rngs)\n",
    "\n",
    "        # Final Layers\n",
    "        self.groupnorm = nnx.GroupNorm(num_features=128, num_groups=32, rngs=rngs)\n",
    "        self.conv_out = nnx.Conv(in_features=128, out_features=3, kernel_size=(3, 3), padding=1, rngs=rngs)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (B, H/8, W/8, C=4)\n",
    "\n",
    "        Returns:\n",
    "            decoded latent (x): (B, H, W, C=3)\n",
    "        \"\"\"\n",
    "        x /= 0.18215  # Unscale\n",
    "\n",
    "        # Define where to upsample\n",
    "        upsamplings = {7, 11, 15}\n",
    "\n",
    "        # List of modules\n",
    "        modules = [self.conv1, self.conv2, self.res1, self.attention,\n",
    "                   self.res2, self.res3, self.res4, self.res5,\n",
    "                   self.conv3, self.res6, self.res7, self.res8,\n",
    "                   self.conv4, self.res9, self.res10, self.res11,\n",
    "                   self.conv5, self.res12, self.res13, self.res14]\n",
    "\n",
    "        for i, module in enumerate(modules):\n",
    "            x = module(x)\n",
    "            if i in upsamplings:\n",
    "                x = upsample(x, scale_factor=2)\n",
    "\n",
    "        x = self.groupnorm(x)\n",
    "        x = nnx.silu(x)\n",
    "        x = self.conv_out(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "def pad_asymmetric(x, padding):\n",
    "    pad_height = padding[0]\n",
    "    pad_width = padding[1]\n",
    "    x = jnp.pad(x, ((0, 0), (pad_height, pad_height), (pad_width, pad_width), (0, 0)), mode='constant')\n",
    "    return x\n",
    "\n",
    "def upsample(x, scale_factor):\n",
    "    B, H, W, C = jnp.shape(x)\n",
    "    out_shape = (B, H*scale_factor, W*scale_factor, C)\n",
    "    x = jax.image.resize(x, out_shape, method=\"nearest\")\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c9ce4da0-70af-47a3-9588-1cd34d761327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder shape test passed!\n"
     ]
    }
   ],
   "source": [
    "def test_encoder_shape():\n",
    "    B, H, W, C = 8, 256, 256, 3  \n",
    "    noise_channels = 4 \n",
    "\n",
    "    rngs = nnx.Rngs({'params': jax.random.PRNGKey(0)})\n",
    "    encoder = Encoder(rngs=rngs)\n",
    "    x = jax.random.normal(jax.random.PRNGKey(1), (B, H, W, C))  # Input image tensor\n",
    "    noise = jax.random.normal(jax.random.PRNGKey(2), (B, H // 8, W // 8, noise_channels))  # Noise tensor\n",
    "    latent = encoder(x, noise)\n",
    "    assert latent.shape == (B, H // 8, W // 8, noise_channels), f\"Expected shape {(B, H // 8, W // 8, noise_channels)}, but got {latent.shape}\"\n",
    "\n",
    "    print(\"Encoder shape test passed!\")\n",
    "\n",
    "test_encoder_shape()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1ed2c0be-f0b7-43a1-b4d3-dd6256ec2e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder shape test passed!\n"
     ]
    }
   ],
   "source": [
    "def test_decoder_shape():\n",
    "    B, H, W, C = 8, 32, 32, 4 \n",
    "    rngs = nnx.Rngs({'params': jax.random.PRNGKey(0)})\n",
    "    decoder = Decoder(rngs=rngs)\n",
    "    x = jax.random.normal(jax.random.PRNGKey(1), (B, H, W, C))\n",
    "    decoded = decoder(x)\n",
    "    expected_H = H * 8\n",
    "    expected_W = W * 8\n",
    "    expected_C = 3 \n",
    "    assert decoded.shape == (B, expected_H, expected_W, expected_C), f\"Expected shape {(B, expected_H, expected_W, expected_C)}, but got {decoded.shape}\"\n",
    "\n",
    "    print(\"Decoder shape test passed!\")\n",
    "\n",
    "test_decoder_shape()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12faadd4-b947-4870-8825-2e4dd5069fe7",
   "metadata": {},
   "source": [
    "# CLIP test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c4ad616e-7e6b-4564-8c4e-d13a7521a9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "class CLIPEmbedding(nnx.Module):\n",
    "    def __init__(self, V: int, D: int, T: int, rngs: nnx.Rngs):\n",
    "        # Token embeddings: shape (V, D)\n",
    "        self.token_embedding_VD = nnx.Embed(\n",
    "            num_embeddings=V, features=D, rngs=rngs\n",
    "        )\n",
    "        # Positional embeddings: shape (T, D)\n",
    "        self.position_embedding_TD = nnx.Param(\n",
    "            jnp.zeros((T, D))\n",
    "        )\n",
    "\n",
    "    def __call__(self, tokens_BL):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tokens_BL: Input token IDs, shape (B, L)\n",
    "        Returns:\n",
    "            embeddings_BLD: Token embeddings with positional encoding, shape (B, L, D)\n",
    "        \"\"\"\n",
    "        embeddings_BLD = self.token_embedding_VD(tokens_BL)  # Shape: (B, L, D)\n",
    "        position_embeddings_LD = self.position_embedding_TD[:embeddings_BLD.shape[1], :]  # Shape: (L, D)\n",
    "        embeddings_BLD += position_embeddings_LD  # Broadcasting over batch dimension\n",
    "        return embeddings_BLD\n",
    "\n",
    "class SelfAttention(nnx.Module):\n",
    "    def __init__(self, H: int, D: int, rngs: nnx.Rngs):\n",
    "        self.H = H  # Number of attention heads\n",
    "        self.D = D  # Embedding dimension\n",
    "        self.K = D // H  # Dimension per head\n",
    "\n",
    "        assert D % H == 0, \"Embedding dimension D must be divisible by number of heads H\"\n",
    "\n",
    "        rng_qkv, rng_out = rngs.split(2)\n",
    "        self.qkv_proj_D3D = nnx.Linear(\n",
    "            in_features=D, out_features=3 * D, use_bias=False, rngs=rng_qkv\n",
    "        )\n",
    "        self.out_proj_DD = nnx.Linear(\n",
    "            in_features=D, out_features=D, rngs=rng_out\n",
    "        )\n",
    "\n",
    "    def __call__(self, input_BLD, causal_mask=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_BLD: Input tensor, shape (B, L, D)\n",
    "            causal_mask: Whether to apply causal masking\n",
    "        Returns:\n",
    "            output_BLD: Output tensor after self-attention, shape (B, L, D)\n",
    "        \"\"\"\n",
    "        B, L, D = input_BLD.shape\n",
    "        H, K = self.H, self.K\n",
    "\n",
    "        qkv_BL3D = self.qkv_proj_D3D(input_BLD)  # Shape: (B, L, 3*D)\n",
    "        qkv_BL3HK = qkv_BL3D.reshape(B, L, 3, H, K)  # Shape: (B, L, 3, H, K)\n",
    "        qkv_3BHLD = qkv_BL3HK.transpose(2, 0, 3, 1, 4)  # Shape: (3, B, H, L, K)\n",
    "        q_BHLK, k_BHLK, v_BHLK = qkv_3BHLD  # Each has shape: (B, H, L, K)\n",
    "\n",
    "        # Compute attention scores\n",
    "        attn_logits_BHLL = jnp.einsum(\"BHLK,BHMK->BHLM\", q_BHLK, k_BHLK) / jnp.sqrt(K)\n",
    "        # attn_logits_BHLL: (B, H, L, L)\n",
    "\n",
    "        if causal_mask:\n",
    "            mask_LL = jnp.tril(jnp.ones((L, L), dtype=bool))\n",
    "            attn_logits_BHLL = jnp.where(mask_LL, attn_logits_BHLL, -1e10)\n",
    "\n",
    "        attn_weights_BHLL = nnx.softmax(attn_logits_BHLL, axis=-1)\n",
    "        attn_output_BHLK = jnp.einsum(\"BHLM,BHMK->BHLK\", attn_weights_BHLL, v_BHLK)\n",
    "        attn_output_BLHK = attn_output_BHLK.transpose(0, 2, 1, 3)  # Shape: (B, L, H, K)\n",
    "        attn_output_BLD = attn_output_BLHK.reshape(B, L, D)  # Shape: (B, L, D)\n",
    "\n",
    "        output_BLD = self.out_proj_DD(attn_output_BLD)  # Shape: (B, L, D)\n",
    "        return output_BLD\n",
    "\n",
    "class CLIPLayer(nnx.Module):\n",
    "    def __init__(self, H: int, D: int, rngs: nnx.Rngs):\n",
    "        # Pre-attention layer normalization\n",
    "        self.layernorm_1 = nnx.LayerNorm()\n",
    "\n",
    "        # Self-attention module\n",
    "        self.attention = SelfAttention(H, D, rngs=rngs)\n",
    "\n",
    "        # Pre-FFN layer normalization\n",
    "        self.layernorm_2 = nnx.LayerNorm()\n",
    "\n",
    "        # Feedforward layers\n",
    "        self.linear_1_DF = nnx.Linear(\n",
    "            in_features=D, out_features=4 * D, rngs=rngs\n",
    "        )  # Shape: (D, F)\n",
    "        self.linear_2_FD = nnx.Linear(\n",
    "            in_features=4 * D, out_features=D, rngs=rngs\n",
    "        )  # Shape: (F, D)\n",
    "\n",
    "    def __call__(self, input_BLD):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_BLD: Input tensor, shape (B, L, D)\n",
    "        Returns:\n",
    "            output_BLD: Output tensor, shape (B, L, D)\n",
    "        \"\"\"\n",
    "        # Residual connection for self-attention\n",
    "        residue_BLD = input_BLD\n",
    "\n",
    "        # Pre-Attention LayerNorm\n",
    "        normalized_BLD = self.layernorm_1(input_BLD)\n",
    "\n",
    "        # Self-Attention\n",
    "        attn_out_BLD = self.attention(normalized_BLD, causal_mask=True)\n",
    "\n",
    "        # Add residual connection\n",
    "        x_BLD = attn_out_BLD + residue_BLD\n",
    "\n",
    "        # Residual connection for feedforward network\n",
    "        residue_BLD = x_BLD\n",
    "\n",
    "        # Pre-FFN LayerNorm\n",
    "        normalized_BLD = self.layernorm_2(x_BLD)\n",
    "\n",
    "        # Feedforward network\n",
    "        ff1_BLF = self.linear_1_DF(normalized_BLD)  # Shape: (B, L, F)\n",
    "        # QuickGELU activation\n",
    "        ff1_BLF = ff1_BLF * nnx.sigmoid(1.702 * ff1_BLF)\n",
    "        ff2_BLD = self.linear_2_FD(ff1_BLF)  # Shape: (B, L, D)\n",
    "\n",
    "        # Add residual connection\n",
    "        output_BLD = ff2_BLD + residue_BLD\n",
    "\n",
    "        return output_BLD\n",
    "\n",
    "class CLIP(nnx.Module):\n",
    "    def __init__(self, V: int, D: int, T: int, H: int, n_layer: int, rngs: nnx.Rngs):\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embedding = CLIPEmbedding(V, D, T, rngs=rngs)\n",
    "\n",
    "        # Encoder layers\n",
    "        self.layers: List[CLIPLayer] = []\n",
    "        for i in range(n_layer):\n",
    "            self.layers.append(CLIPLayer(H, D, rngs=rngs))\n",
    "\n",
    "        # Final layer normalization\n",
    "        self.layernorm = nnx.LayerNorm()\n",
    "\n",
    "    def __call__(self, tokens_BL):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tokens_BL: Input token IDs, shape (B, L)\n",
    "        Returns:\n",
    "            output_BLD: Output tensor, shape (B, L, D)\n",
    "        \"\"\"\n",
    "        tokens_BL = tokens_BL.astype(jnp.int32)\n",
    "\n",
    "        # Embedding\n",
    "        x_BLD = self.embedding(tokens_BL)\n",
    "\n",
    "        # Apply encoder layers\n",
    "        for layer in self.layers:\n",
    "            x_BLD = layer(x_BLD)\n",
    "\n",
    "        # Final layer normalization\n",
    "        output_BLD = self.layernorm(x_BLD)\n",
    "\n",
    "        return output_BLD  # Shape: (B, L, D)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e8655bcd-aed4-4d93-907d-eaadefbea4bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "LayerNorm.__init__() missing 1 required positional argument: 'num_features'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[104], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m n_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m12\u001b[39m  \u001b[38;5;66;03m# Number of layers\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Instantiate the model\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mCLIP\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mV\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mV\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mD\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mD\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mT\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mH\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_layer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrngs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrngs\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Example input tokens_BL (batch size B=2, sequence length L=77)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m tokens_BL \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39marray([\n\u001b[1;32m     22\u001b[0m     [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m+\u001b[39m [\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m (T \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m3\u001b[39m),\n\u001b[1;32m     23\u001b[0m     [\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m6\u001b[39m] \u001b[38;5;241m+\u001b[39m [\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m (T \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     24\u001b[0m ])  \u001b[38;5;66;03m# Shape: (B=2, L=77)\u001b[39;00m\n",
      "File \u001b[0;32m~/Github/StableDiffusax/env/stablediffusax/lib/python3.11/site-packages/flax/nnx/nnx/object.py:79\u001b[0m, in \u001b[0;36mObjectMeta.__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m---> 79\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_graph_node_meta_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Github/StableDiffusax/env/stablediffusax/lib/python3.11/site-packages/flax/nnx/nnx/object.py:88\u001b[0m, in \u001b[0;36m_graph_node_meta_call\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28mvars\u001b[39m(node)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_object__state\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m ObjectState()\n\u001b[0;32m---> 88\u001b[0m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_object_meta_construct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m node\n",
      "File \u001b[0;32m~/Github/StableDiffusax/env/stablediffusax/lib/python3.11/site-packages/flax/nnx/nnx/object.py:82\u001b[0m, in \u001b[0;36mObjectMeta._object_meta_construct\u001b[0;34m(cls, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_object_meta_construct\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 82\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[103], line 138\u001b[0m, in \u001b[0;36mCLIP.__init__\u001b[0;34m(self, V, D, T, H, n_layer, rngs)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers: List[CLIPLayer] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_layer):\n\u001b[0;32m--> 138\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mappend(\u001b[43mCLIPLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrngs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrngs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    140\u001b[0m \u001b[38;5;66;03m# Final layer normalization\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayernorm \u001b[38;5;241m=\u001b[39m nnx\u001b[38;5;241m.\u001b[39mLayerNorm()\n",
      "File \u001b[0;32m~/Github/StableDiffusax/env/stablediffusax/lib/python3.11/site-packages/flax/nnx/nnx/object.py:79\u001b[0m, in \u001b[0;36mObjectMeta.__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m---> 79\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_graph_node_meta_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Github/StableDiffusax/env/stablediffusax/lib/python3.11/site-packages/flax/nnx/nnx/object.py:88\u001b[0m, in \u001b[0;36m_graph_node_meta_call\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28mvars\u001b[39m(node)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_object__state\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m ObjectState()\n\u001b[0;32m---> 88\u001b[0m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_object_meta_construct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m node\n",
      "File \u001b[0;32m~/Github/StableDiffusax/env/stablediffusax/lib/python3.11/site-packages/flax/nnx/nnx/object.py:82\u001b[0m, in \u001b[0;36mObjectMeta._object_meta_construct\u001b[0;34m(cls, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_object_meta_construct\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 82\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[103], line 77\u001b[0m, in \u001b[0;36mCLIPLayer.__init__\u001b[0;34m(self, H, D, rngs)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, H: \u001b[38;5;28mint\u001b[39m, D: \u001b[38;5;28mint\u001b[39m, rngs: nnx\u001b[38;5;241m.\u001b[39mRngs):\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;66;03m# Pre-attention layer normalization\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayernorm_1 \u001b[38;5;241m=\u001b[39m \u001b[43mnnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLayerNorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;66;03m# Self-attention module\u001b[39;00m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention \u001b[38;5;241m=\u001b[39m SelfAttention(H, D, rngs\u001b[38;5;241m=\u001b[39mrngs)\n",
      "File \u001b[0;32m~/Github/StableDiffusax/env/stablediffusax/lib/python3.11/site-packages/flax/nnx/nnx/object.py:79\u001b[0m, in \u001b[0;36mObjectMeta.__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m---> 79\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_graph_node_meta_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Github/StableDiffusax/env/stablediffusax/lib/python3.11/site-packages/flax/nnx/nnx/object.py:88\u001b[0m, in \u001b[0;36m_graph_node_meta_call\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28mvars\u001b[39m(node)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_object__state\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m ObjectState()\n\u001b[0;32m---> 88\u001b[0m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_object_meta_construct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m node\n",
      "File \u001b[0;32m~/Github/StableDiffusax/env/stablediffusax/lib/python3.11/site-packages/flax/nnx/nnx/object.py:82\u001b[0m, in \u001b[0;36mObjectMeta._object_meta_construct\u001b[0;34m(cls, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_object_meta_construct\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 82\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: LayerNorm.__init__() missing 1 required positional argument: 'num_features'"
     ]
    }
   ],
   "source": [
    "rngs = nnx.Rngs({'params': jax.random.PRNGKey(0)})\n",
    "\n",
    "# Define model parameters\n",
    "V = 49408     # Vocabulary size\n",
    "D = 768       # Embedding dimension\n",
    "T = 77        # Maximum sequence length\n",
    "H = 12        # Number of attention heads\n",
    "n_layer = 12  # Number of layers\n",
    "\n",
    "# Instantiate the model\n",
    "model = CLIP(\n",
    "    V=V,\n",
    "    D=D,\n",
    "    T=T,\n",
    "    H=H,\n",
    "    n_layer=n_layer,\n",
    "    rngs=rngs\n",
    ")\n",
    "\n",
    "# Example input tokens_BL (batch size B=2, sequence length L=77)\n",
    "tokens_BL = jnp.array([\n",
    "    [1, 2, 3] + [0] * (T - 3),\n",
    "    [4, 5, 6] + [0] * (T - 3)\n",
    "])  # Shape: (B=2, L=77)\n",
    "\n",
    "# Forward pass\n",
    "output_BLD = model(tokens_BL)  # Output shape: (B=2, L=77, D=768)\n",
    "print(f\"Output shape: {output_BLD.shape}\")  # Should print (2, 77, 768)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021bb98f-d0fc-43bc-95b0-670716220332",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusax",
   "language": "python",
   "name": "stablediffusax"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
