{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "44859ca3-13a5-44de-a2e8-359ffc3e1d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.nnx as nnx\n",
    "import einops\n",
    "\n",
    "\"\"\"\n",
    "Dimension key:\n",
    "B: batch size\n",
    "L: sequence length\n",
    "D: embedding dimension\n",
    "H: number of attention heads\n",
    "K: size of each attention head (D // H)\n",
    "\"\"\"\n",
    "\n",
    "class SelfAttention(nnx.Module):\n",
    "    def __init__(self, n_heads: int, d_embed: int, rngs: nnx.Rngs, in_proj_bias: bool = True, out_proj_bias: bool = True):\n",
    "        self.n_heads = n_heads\n",
    "        self.d_embed = d_embed\n",
    "        self.d_head = d_embed // n_heads\n",
    "        self.in_proj = nnx.Linear(in_features=d_embed, out_features=3*d_embed, use_bias=in_proj_bias, rngs=rngs)\n",
    "        self.out_proj = nnx.Linear(in_features=d_embed, out_features=d_embed, use_bias=out_proj_bias, rngs=rngs)\n",
    "\n",
    "    def __call__(self, x_BLD,  causal_mask=False):\n",
    "        B, L, D = jnp.shape(x_BLD)\n",
    "        H = self.n_heads\n",
    "        K = self.d_head # K = D // H\n",
    "\n",
    "        qkv_BL3D = self.in_proj(x_BLD) # (B, L, 3D)\n",
    "        q_BLD, k_BLD, v_BLD = jnp.split(qkv_BL3D, 3, axis=-1)\n",
    "\n",
    "        q_BHLK = einops.rearrange(q_BLD, 'B L (H K) -> B H L K', H=H)\n",
    "        k_BHLK = einops.rearrange(k_BLD, 'B L (H K) -> B H L K', H=H)\n",
    "        v_BHLK = einops.rearrange(v_BLD, 'B L (H K) -> B H L K', H=H)\n",
    "\n",
    "        attn_logits_BHLL = jnp.einsum('BHLK,BHMK->BHLM', q_BHLK, k_BHLK) / jnp.sqrt(K)\n",
    "\n",
    "        if causal_mask:\n",
    "            mask_LL = jnp.triu(jnp.ones((L, L)), k=1).astype(bool)\n",
    "            attn_logits_BHLL = jnp.where(mask_LL, -jnp.inf, attn_logits_BHLL)\n",
    "\n",
    "        attn_weights_BHLL = nnx.softmax(attn_logits_BHLL, axis=-1)\n",
    "\n",
    "        out_BHLK = jnp.einsum('BHLM,BHMK->BHLK', attn_logits_BHLL, v_BHLK)\n",
    "\n",
    "        out_BLD = einops.rearrange(out_BHLK, 'B H L K -> B L (H K)')\n",
    "\n",
    "        out_BLD = self.out_proj(out_BLD)\n",
    "\n",
    "        return out_BLD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000b5b67-ffdc-458f-8155-8478b0119e8a",
   "metadata": {},
   "source": [
    "# Test attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "dab216c4-bad5-41e2-a5d5-eb71de104714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfAttention shape test passed!\n"
     ]
    }
   ],
   "source": [
    "def test_self_attention_shape():\n",
    "    B, L, D, H = 8, 10, 64, 8\n",
    "    rngs = nnx.Rngs({'params': jax.random.PRNGKey(0)})\n",
    "\n",
    "    attention_layer = SelfAttention(n_heads=H, d_embed=D, rngs=rngs)\n",
    "    x_BLD = jax.random.normal(jax.random.PRNGKey(1), (B, L, D))\n",
    "    output = attention_layer(x_BLD, causal_mask=True)\n",
    "    assert output.shape == (B, L, D), f\"Expected shape {(B, L, D)}, but got {output.shape}\"\n",
    "    print(\"SelfAttention shape test passed!\")\n",
    "\n",
    "# Run the shape test\n",
    "test_self_attention_shape()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "45063b19-3909-4411-958c-aa203b7b9ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flax.nnx as nnx\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import einops\n",
    "\n",
    "\"\"\"\n",
    "Dimension key:\n",
    "(missing!)\n",
    "\"\"\"\n",
    "\n",
    "class ResidualBlock:\n",
    "    def __init__(self, in_features: int, out_features: int, rngs: nnx.Rngs):\n",
    "        self.gn1 = nnx.GroupNorm(num_features=in_features, num_groups=32, rngs=rngs)\n",
    "        self.conv1 = nnx.Conv(in_features=in_features, out_features=out_features, kernel_size=(3,3), padding=1, rngs=rngs)\n",
    "\n",
    "        self.gn2 = nnx.GroupNorm(num_features=out_features, num_groups=32, rngs=rngs)\n",
    "        self.conv2 = nnx.Conv(in_features=out_features, out_features=out_features, kernel_size=(3,3), padding=1, rngs=rngs)\n",
    "\n",
    "        if in_features != out_features:\n",
    "            self.shortcut = nnx.Conv(in_features=in_features, out_features=out_features, kernel_size=(1,1), padding=0, rngs=rngs)\n",
    "        else:\n",
    "            self.shortcut = lambda x: x\n",
    "\n",
    "    def __call__(self, x):\n",
    "        residual = x\n",
    "\n",
    "        x = self.gn1(x)\n",
    "        x = nnx.silu(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.gn2(x)\n",
    "        x = nnx.silu(x)\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        return x + self.shortcut(residual)\n",
    "\n",
    "class AttentionBlock:\n",
    "    def __init__(self, num_features: int, rngs: nnx.Rngs):\n",
    "        self.gn = nnx.GroupNorm(num_features=num_features, num_groups=32, rngs=rngs)\n",
    "        self.attention = SelfAttention(n_heads=1, d_embed=num_features, rngs=rngs)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # recall that JAX expects (B, H, W, C)\n",
    "        residual = x\n",
    "\n",
    "        x = self.gn(x)\n",
    "\n",
    "        B, H, W, C = jnp.shape(x)\n",
    "\n",
    "        x = einops.rearrange(x, 'B H W C -> B (H W) C')\n",
    "        x = self.attention(x)\n",
    "        x = einops.rearrange(x, 'B (H W) C -> B H W C', H=H, W=W)\n",
    "        x += residual\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e082b0-0ab4-4054-ab60-504c2dc469c2",
   "metadata": {},
   "source": [
    "# Test residual block and attention block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5bc82637-ec8d-4e50-bf2a-34e87d19aee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResidualBlock shape test passed!\n"
     ]
    }
   ],
   "source": [
    "def test_residual_block_shape():\n",
    "    B, H, W, in_features, out_features = 8, 32, 32, 64, 128  # Example: batch size 8, height and width 32, input channels 64, output channels 128\n",
    "    rngs = nnx.Rngs({'params': jax.random.PRNGKey(0)})\n",
    "    \n",
    "    # Instantiate the ResidualBlock\n",
    "    residual_block = ResidualBlock(in_features=in_features, out_features=out_features, rngs=rngs)\n",
    "\n",
    "    # Random input tensor with shape (B, H, W, in_features)\n",
    "    x = jax.random.normal(jax.random.PRNGKey(1), (B, H, W, in_features))\n",
    "\n",
    "    # Forward pass through the ResidualBlock\n",
    "    output = residual_block(x)\n",
    "\n",
    "    # Check the shape of the output\n",
    "    assert output.shape == (B, H, W, out_features), f\"Expected shape {(B, H, W, out_features)}, but got {output.shape}\"\n",
    "\n",
    "    print(\"ResidualBlock shape test passed!\")\n",
    "\n",
    "# Run the shape test\n",
    "test_residual_block_shape()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4d7406ab-24bb-46fe-abf9-dfecff8cd27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AttentionBlock shape test passed!\n"
     ]
    }
   ],
   "source": [
    "def test_attention_block_shape():\n",
    "    B, H, W, C = 8, 32, 32, 64 \n",
    "    rngs = nnx.Rngs({'params': jax.random.PRNGKey(0)})\n",
    "    attention_block = AttentionBlock(num_features=C, rngs=rngs)\n",
    "    x = jax.random.normal(jax.random.PRNGKey(1), (B, H, W, C))\n",
    "    output = attention_block(x)\n",
    "    assert output.shape == (B, H, W, C), f\"Expected shape {(B, H, W, C)}, but got {output.shape}\"\n",
    "\n",
    "    print(\"AttentionBlock shape test passed!\")\n",
    "\n",
    "# Run the shape test\n",
    "test_attention_block_shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6a5693-12d9-419b-986c-1de222ec43a4",
   "metadata": {},
   "source": [
    "# Test encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "cbcb6b07-f02c-4467-b861-2c4a197058f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nnx.Module):\n",
    "    def __init__(self, rngs: nnx.Rngs):\n",
    "        # 3 -> 128 -> (6 x 128)\n",
    "        self.conv1 = nnx.Conv(in_features=3, out_features=128, kernel_size=(3, 3), padding=1, rngs=rngs)\n",
    "\n",
    "        self.res1 = ResidualBlock(in_features=128, out_features=128, rngs=rngs)\n",
    "        self.res2 = ResidualBlock(in_features=128, out_features=128, rngs=rngs)\n",
    "\n",
    "        self.conv2 = nnx.Conv(in_features=128, out_features=128, kernel_size=(3, 3), strides=(2, 2), padding=0, rngs=rngs)\n",
    "\n",
    "        # 128 -> 256 -> (4 x 256)\n",
    "        self.res3 = ResidualBlock(in_features=128, out_features=256, rngs=rngs)\n",
    "        self.res4 = ResidualBlock(in_features=256, out_features=256, rngs=rngs)\n",
    "\n",
    "        self.conv3 = nnx.Conv(in_features=256, out_features=256, kernel_size=(3, 3), strides=(2, 2), padding=0, rngs=rngs)\n",
    "\n",
    "        # 256 -> 512 -> (10 x 512)\n",
    "        self.res5 = ResidualBlock(in_features=256, out_features=512, rngs=rngs)\n",
    "        self.res6 = ResidualBlock(in_features=512, out_features=512, rngs=rngs)\n",
    "\n",
    "        self.conv4 = nnx.Conv(in_features=512, out_features=512, kernel_size=(3, 3), strides=(2, 2), padding=0, rngs=rngs)\n",
    "\n",
    "        self.res7 = ResidualBlock(in_features=512, out_features=512, rngs=rngs)\n",
    "        self.res8 = ResidualBlock(in_features=512, out_features=512, rngs=rngs)\n",
    "        self.res9 = ResidualBlock(in_features=512, out_features=512, rngs=rngs)\n",
    "\n",
    "        self.attention = AttentionBlock(num_features=512, rngs=rngs)\n",
    "\n",
    "        # 512 -> 512\n",
    "        self.res10 = ResidualBlock(in_features=512, out_features=512, rngs=rngs)\n",
    "\n",
    "        self.groupnorm = nnx.GroupNorm(num_features=512, num_groups=32, rngs=rngs)\n",
    "\n",
    "        # 512 -> 8 -> (2 x 8)\n",
    "        self.conv5 = nnx.Conv(in_features=512, out_features=8, kernel_size=(3, 3), padding=1, rngs=rngs)\n",
    "        self.conv6 = nnx.Conv(in_features=8, out_features=8, kernel_size=(1, 1), padding=0, rngs=rngs)\n",
    "\n",
    "    def __call__(self, x, noise):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        x: (B, H, W, C=3)\n",
    "        noise: (B, H/8, W/8, C=4)\n",
    "\n",
    "        Returns:\n",
    "        latent (z): (B, H/8, W/8, 4)\n",
    "        \"\"\"\n",
    "\n",
    "        # Q: is there a better way to do this?\n",
    "        for module in [self.conv1, self.res1, self.res2, self.conv2, self.res3, self.res4, self.conv3,\n",
    "                       self.res5, self.res6, self.conv4, self.res7, self.res8, self.res9, self.attention,\n",
    "                       self.res10, self.groupnorm]:\n",
    "\n",
    "            if isinstance(module, nnx.Conv) and module.strides == (2, 2):\n",
    "                x = pad_asymmetric(x,  padding=(1, 1))\n",
    "            x = module(x)\n",
    "\n",
    "        x = nnx.silu(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.conv6(x)\n",
    "\n",
    "        if x.shape[-1] % 2 != 0:\n",
    "            raise ValueError(\"Final output channels must be divisible by 2 to split mean and log_var\")\n",
    "\n",
    "        mean, log_var = jnp.split(x, 2, axis=-1)\n",
    "        log_var = jnp.clip(log_var, -30, 20)\n",
    "        variance = jnp.exp(log_var)\n",
    "        stdev = jnp.sqrt(variance)\n",
    "\n",
    "        x = mean + stdev * noise\n",
    "        x *= 0.18215\n",
    "\n",
    "        return x\n",
    "\n",
    "class Decoder(nnx.Module):\n",
    "    def __init__(self, rngs: nnx.Rngs):\n",
    "        # Initial Convolutions and Blocks\n",
    "        self.conv1 = nnx.Conv(in_features=4, out_features=4, kernel_size=(1, 1), padding=0, rngs=rngs)\n",
    "        self.conv2 = nnx.Conv(in_features=4, out_features=512, kernel_size=(3, 3), padding=1, rngs=rngs)\n",
    "        self.res1 = ResidualBlock(in_features=512, out_features=512, rngs=rngs)\n",
    "        self.attention = AttentionBlock(num_features=512, rngs=rngs)\n",
    "        self.res2 = ResidualBlock(in_features=512, out_features=512, rngs=rngs)\n",
    "        self.res3 = ResidualBlock(in_features=512, out_features=512, rngs=rngs)\n",
    "        self.res4 = ResidualBlock(in_features=512, out_features=512, rngs=rngs)\n",
    "        self.res5 = ResidualBlock(in_features=512, out_features=512, rngs=rngs)\n",
    "\n",
    "        # Upsampling Blocks\n",
    "        self.conv3 = nnx.Conv(in_features=512, out_features=512, kernel_size=(3, 3), padding=1, rngs=rngs)\n",
    "        self.res6 = ResidualBlock(in_features=512, out_features=512, rngs=rngs)\n",
    "        self.res7 = ResidualBlock(in_features=512, out_features=512, rngs=rngs)\n",
    "        self.res8 = ResidualBlock(in_features=512, out_features=512, rngs=rngs)\n",
    "\n",
    "        # Upsampling Blocks\n",
    "        self.conv4 = nnx.Conv(in_features=512, out_features=256, kernel_size=(3, 3), padding=1, rngs=rngs)\n",
    "        self.res9 = ResidualBlock(in_features=256, out_features=256, rngs=rngs)\n",
    "        self.res10 = ResidualBlock(in_features=256, out_features=256, rngs=rngs)\n",
    "        self.res11 = ResidualBlock(in_features=256, out_features=256, rngs=rngs)\n",
    "\n",
    "        # Upsampling Blocks\n",
    "        self.conv5 = nnx.Conv(in_features=256, out_features=128, kernel_size=(3, 3), padding=1, rngs=rngs)\n",
    "        self.res12 = ResidualBlock(in_features=128, out_features=128, rngs=rngs)\n",
    "        self.res13 = ResidualBlock(in_features=128, out_features=128, rngs=rngs)\n",
    "        self.res14 = ResidualBlock(in_features=128, out_features=128, rngs=rngs)\n",
    "\n",
    "        # Final Layers\n",
    "        self.groupnorm = nnx.GroupNorm(num_features=128, num_groups=32, rngs=rngs)\n",
    "        self.conv_out = nnx.Conv(in_features=128, out_features=3, kernel_size=(3, 3), padding=1, rngs=rngs)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (B, H/8, W/8, C=4)\n",
    "\n",
    "        Returns:\n",
    "            decoded latent (x): (B, H, W, C=3)\n",
    "        \"\"\"\n",
    "        x /= 0.18215  # Unscale\n",
    "\n",
    "        # Define where to upsample\n",
    "        upsamplings = {7, 11, 15}\n",
    "\n",
    "        # List of modules\n",
    "        modules = [self.conv1, self.conv2, self.res1, self.attention,\n",
    "                   self.res2, self.res3, self.res4, self.res5,\n",
    "                   self.conv3, self.res6, self.res7, self.res8,\n",
    "                   self.conv4, self.res9, self.res10, self.res11,\n",
    "                   self.conv5, self.res12, self.res13, self.res14]\n",
    "\n",
    "        for i, module in enumerate(modules):\n",
    "            x = module(x)\n",
    "            if i in upsamplings:\n",
    "                x = upsample(x, scale_factor=2)\n",
    "\n",
    "        x = self.groupnorm(x)\n",
    "        x = nnx.silu(x)\n",
    "        x = self.conv_out(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "def pad_asymmetric(x, padding):\n",
    "    pad_height = padding[0]\n",
    "    pad_width = padding[1]\n",
    "    x = jnp.pad(x, ((0, 0), (pad_height, pad_height), (pad_width, pad_width), (0, 0)), mode='constant')\n",
    "    return x\n",
    "\n",
    "def upsample(x, scale_factor):\n",
    "    B, H, W, C = jnp.shape(x)\n",
    "    out_shape = (B, H*scale_factor, W*scale_factor, C)\n",
    "    x = jax.image.resize(x, out_shape, method=\"nearest\")\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c9ce4da0-70af-47a3-9588-1cd34d761327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder shape test passed!\n"
     ]
    }
   ],
   "source": [
    "def test_encoder_shape():\n",
    "    B, H, W, C = 8, 256, 256, 3  \n",
    "    noise_channels = 4 \n",
    "\n",
    "    rngs = nnx.Rngs({'params': jax.random.PRNGKey(0)})\n",
    "    encoder = Encoder(rngs=rngs)\n",
    "    x = jax.random.normal(jax.random.PRNGKey(1), (B, H, W, C))  # Input image tensor\n",
    "    noise = jax.random.normal(jax.random.PRNGKey(2), (B, H // 8, W // 8, noise_channels))  # Noise tensor\n",
    "    latent = encoder(x, noise)\n",
    "    assert latent.shape == (B, H // 8, W // 8, noise_channels), f\"Expected shape {(B, H // 8, W // 8, noise_channels)}, but got {latent.shape}\"\n",
    "\n",
    "    print(\"Encoder shape test passed!\")\n",
    "\n",
    "test_encoder_shape()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1ed2c0be-f0b7-43a1-b4d3-dd6256ec2e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder shape test passed!\n"
     ]
    }
   ],
   "source": [
    "def test_decoder_shape():\n",
    "    B, H, W, C = 8, 32, 32, 4 \n",
    "    rngs = nnx.Rngs({'params': jax.random.PRNGKey(0)})\n",
    "    decoder = Decoder(rngs=rngs)\n",
    "    x = jax.random.normal(jax.random.PRNGKey(1), (B, H, W, C))\n",
    "    decoded = decoder(x)\n",
    "    expected_H = H * 8\n",
    "    expected_W = W * 8\n",
    "    expected_C = 3 \n",
    "    assert decoded.shape == (B, expected_H, expected_W, expected_C), f\"Expected shape {(B, expected_H, expected_W, expected_C)}, but got {decoded.shape}\"\n",
    "\n",
    "    print(\"Decoder shape test passed!\")\n",
    "\n",
    "test_decoder_shape()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a19e5b-9ce1-4c1b-8c62-02a61715942f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusax",
   "language": "python",
   "name": "stablediffusax"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
